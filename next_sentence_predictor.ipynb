{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is GPU or not\n",
    "!nvidia-smi\n",
    "# Install tensorflow 2.3.0\n",
    "!pip install -q tensorflow==2.3.0\n",
    "# Clone the TensorFlow models Repo\n",
    "!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git\n",
    "!pip install -Uqr models/official/requirements.txt\n",
    "# Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "sys.path.append('models')\n",
    "from official.nlp.data import classifier_data_lib\n",
    "from official.nlp.bert import tokenization\n",
    "from official.nlp import optimization\n",
    "\n",
    "# keras imports\n",
    "from tf.keras.layers import Input, Dropout, Dense\n",
    "from tf.keras.optimizers import Adam\n",
    "from tf.keras.metrics import BinaryAccuracy\n",
    "from tf.keras.losses import BinaryCrossentropy\n",
    "from tf.keras.utils import plot_model\n",
    "from tf.keras.models import Model\n",
    "# Load the Quora Insincrere QUesrtion dataset.\n",
    "df = pd.read_csv(\n",
    "'https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip',\n",
    "\t\t\t\tcompression='zip')\n",
    "df.head()\n",
    "# plot the histogram of sincere and insincere question vs sincere ques\n",
    "df.target.plot(kind='hist', title='Sincere (0) vs Insincere (1) distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation\n",
    "train_df, remaining = train_test_split(df, train_size=0.01,\n",
    "\t\t\t\t\t\t\t\t\tstratify=df.target.values)\n",
    "valid_df, _ = train_test_split(remaining, train_size=0.001,\n",
    "\t\t\t\t\t\t\tstratify=remaining.target.values)\n",
    "train_df.shape, valid_df.shape\n",
    "\n",
    "# import for processing dataset\n",
    "from tf.data.Dataset import from_tensor_slices\n",
    "from tf.data.experimental import AUTOTUNE\n",
    "\n",
    "# convert dataset into tensor slices\n",
    "with tf.device('/cpu:0'):\n",
    "train_data =from_tensor_slices((train_df.question_text.values,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttrain_df.target.values))\n",
    "valid_data = from_tensor_slices((valid_df.question_text.values,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tvalid_df.target.values))\n",
    "\t\n",
    "for text, label in train_data.take(2):\n",
    "\tprint(text)\n",
    "\tprint(label)\n",
    "\t\n",
    "label_list = [0, 1] # Label categories\n",
    "max_seq_length = 128 # maximum length of input sequences\n",
    "train_batch_size = 32\n",
    "\n",
    "# Get BERT layer and tokenizer:\n",
    "bert_layer = hub.KerasLayer(\n",
    "\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "\t\t\t\t\t\t\ttrainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "# example\n",
    "# convert to tokens ids and \n",
    "tokenizer.convert_tokens_to_ids(\n",
    "tokenizer.wordpiece_tokenizer.tokenize('how are you?'))\n",
    "\n",
    "# convert the dataset into the format required by BERT i.e we convert the row into\n",
    "# input features (Token id, input mask, input type id ) and labels\n",
    "\n",
    "def convert_to_bert_feature(text, label, label_list=label_list, \n",
    "\t\t\tmax_seq_length=max_seq_length, tokenizer=tokenizer):\n",
    "example = classifier_data_lib.InputExample(guid = None,\n",
    "\t\t\t\t\t\t\t\t\t\t\ttext_a = text.numpy(), \n",
    "\t\t\t\t\t\t\t\t\t\t\ttext_b = None, \n",
    "\t\t\t\t\t\t\t\t\t\t\tlabel = label.numpy())\n",
    "feature = classifier_data_lib.convert_single_example(0, example, label_list,\n",
    "\t\t\t\t\t\t\t\t\tmax_seq_length, tokenizer)\n",
    "\n",
    "return (feature.input_ids, feature.input_mask, feature.segment_ids, \n",
    "\t\tfeature.label_id)\n",
    "\n",
    "# wrap the dataset around the python function in order to use the tf\n",
    "# datasets map function\n",
    "def to_bert_feature_map(text, label):\n",
    "\n",
    "input_ids, input_mask, segment_ids, label_id = tf.py_function(\n",
    "\tconvert_to_bert_feature,\n",
    "\tinp=[text, label],\n",
    "\tTout=[tf.int32, tf.int32, tf.int32, tf.int32])\n",
    "\n",
    "# py_func doesn't set the shape of the returned tensors.\n",
    "input_ids.set_shape([max_seq_length])\n",
    "input_mask.set_shape([max_seq_length])\n",
    "segment_ids.set_shape([max_seq_length])\n",
    "label_id.set_shape([])\n",
    "\n",
    "x = {\n",
    "\t\t'input_word_ids': input_ids,\n",
    "\t\t'input_mask': input_mask,\n",
    "\t\t'input_type_ids': segment_ids\n",
    "\t}\n",
    "return (x, label_id)\n",
    "with tf.device('/cpu:0'):\n",
    "# train\n",
    "train_data = (train_data.map(to_bert_feature_map,\n",
    "\t\t\t\t\t\t\tnum_parallel_calls=AUTOTUNE)\n",
    "\t\t\t\t\t\t#.cache()\n",
    "\t\t\t\t\t\t.shuffle(1000)\n",
    "\t\t\t\t\t\t.batch(32, drop_remainder=True)\n",
    "\t\t\t\t\t\t.prefetch(AUTOTUNE))\n",
    "\n",
    "# valid\n",
    "valid_data = (valid_data.map(to_bert_feature_map,\n",
    "\t\t\t\t\t\t\tnum_parallel_calls=AUTOTUNE)\n",
    "\t\t\t\t\t\t.batch(32, drop_remainder=True)\n",
    "\t\t\t\t\t\t.prefetch(AUTOTUNE)) \n",
    "\n",
    "# example format train and valid data\n",
    "print(\"train data format\",train_data.element_spec)\n",
    "print(\"validation data format\",valid_data.element_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "# Building the model\n",
    "def fine_tuned_model():\n",
    "input_word_ids = Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "\t\t\t\t\t\t\t\t\tname=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "\t\t\t\t\t\t\t\tname=\"input_mask\")\n",
    "input_type_ids = Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "\t\t\t\t\t\t\t\tname=\"input_type_ids\")\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, \n",
    "\t\t\t\t\t\t\t\t\t\t\tinput_type_ids])\n",
    "\n",
    "drop = Dropout(0.4)(pooled_output)\n",
    "output = Dense(1, activation=\"sigmoid\", name=\"output\")(drop)\n",
    "\n",
    "model = Model(\n",
    "\tinputs={\n",
    "\t\t'input_word_ids': input_word_ids,\n",
    "\t\t'input_mask': input_mask,\n",
    "\t\t'input_type_ids': input_type_ids\n",
    "\t},\n",
    "\toutputs=output)\n",
    "return model\n",
    "\n",
    "#compile the model\n",
    "model = fine_tuned_model()\n",
    "model.compile(optimizer=Adam(learning_rate=2e-5),\n",
    "\t\t\tloss=BinaryCrossentropy(),\n",
    "\t\t\tmetrics=[BinaryAccuracy()])\n",
    "model.summary()\n",
    "#plot the model\n",
    "plot_model(model=model, show_shapes=True)\n",
    "# Train model\n",
    "epochs = 4\n",
    "history = model.fit(train_data,\n",
    "\t\t\t\t\tvalidation_data=valid_data,\n",
    "\t\t\t\t\tepochs=epochs,\n",
    "\t\t\t\t\tverbose=1)\n",
    "# plot the accuracy\n",
    "def plot_graphs(history, metric):\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history['val_'+metric], '')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(metric)\n",
    "plt.legend([metric, 'val_'+metric])\n",
    "plt.show()\n",
    "plot_graphs(history, 'binary_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "test_eg=['what is the current marketprice of petroleum?', \n",
    "\t\t'who is Oswald?', 'why are you here idiot ?']\n",
    "test_data =from_tensor_slices((test_eg, [0]*len(test_eg)))\n",
    "# wrap test data into BERT format\n",
    "test_data = (test_data.map(to_feature_map_bert).batch(1))\n",
    "preds = model.predict(test_data)\n",
    "print(preds)\n",
    "['Insincere' if pred >=0.5 else 'Sincere' for pred in preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
